{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahr\\anaconda3\\envs\\lm-prompt-turbo2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, get_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "# models = [\n",
    "#     'microsoft/deberta-v3-xsmall',\n",
    "#     'microsoft/deberta-v3-small',\n",
    "#     'microsoft/deberta-v3-large',\n",
    "#     'microsoft/deberta-v3-base',\n",
    "# ]\n",
    "\n",
    "MODEL_NAME = 'cross-encoder/nli-deberta-v3-base'\n",
    "# MODEL_NAME = 'cross-encoder/nli-deberta-v3-small'\n",
    "# MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "# MODEL_NAME = \"sileod/deberta-v3-base-tasksource-nli\"\n",
    "# MODEL_NAME = \"clagator/biobert_v1.1_pubmed_nli_sts\"\n",
    "# MODEL_NAME = \"gsarti/scibert-nli\"\n",
    "# MODEL_NAME = \"gsarti/biobert-nli\"\n",
    "# MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.2\" BAD\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\shahr\\anaconda3\\envs\\lm-prompt-turbo2\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: [CLS] Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.Primary premise text here.[SEP] Statement text here.[SEP]\n"
     ]
    }
   ],
   "source": [
    "def debug_tokenization(model_name, primary_premise, secondary_premise, statement, premise_to_use, max_premise_length, max_input_length):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # if premise_to_use == \"Original_truncated\":\n",
    "    #     # Tokenize the primary premise\n",
    "    #     primary_tokens = tokenizer.encode(primary_premise, add_special_tokens=False)\n",
    "    #     individual_premise_length = max_premise_length if not secondary_premise else int(max_premise_length / 2)\n",
    "    #     combined_tokens = primary_tokens[:individual_premise_length]\n",
    "\n",
    "    #     if secondary_premise:\n",
    "    #         # Tokenize the secondary premise\n",
    "    #         secondary_tokens = tokenizer.encode(secondary_premise, add_special_tokens=False)\n",
    "    #         # combined_tokens += [tokenizer.sep_token_id] + secondary_tokens[:individual_premise_length]\n",
    "    #         combined_tokens += secondary_tokens[:individual_premise_length]\n",
    "    # else:  # summarized_premise\n",
    "    #     # Tokenize the summarized premise\n",
    "    #     combined_tokens = tokenizer.encode(primary_premise, add_special_tokens=False)\n",
    "\n",
    "    if premise_to_use == \"Combined\":\n",
    "        # Tokenize the summarized premise\n",
    "        combined_tokens = tokenizer.encode(primary_premise, add_special_tokens=False)\n",
    "    else:\n",
    "        # Tokenize the primary premise\n",
    "        primary_tokens = tokenizer.encode(primary_premise, add_special_tokens=False)\n",
    "        individual_premise_length = max_premise_length if not secondary_premise else int(max_premise_length / 2)\n",
    "        combined_tokens = primary_tokens[:individual_premise_length]\n",
    "\n",
    "        if secondary_premise:\n",
    "            # Tokenize the secondary premise\n",
    "            secondary_tokens = tokenizer.encode(secondary_premise, add_special_tokens=False)\n",
    "            # combined_tokens += [tokenizer.sep_token_id] + secondary_tokens[:individual_premise_length]\n",
    "            combined_tokens += secondary_tokens[:individual_premise_length]\n",
    "\n",
    "    # Tokenize the statement\n",
    "    statement_tokens = tokenizer.encode(statement, add_special_tokens=False)\n",
    "\n",
    "    # Combine tokens for final input\n",
    "    input_tokens = [tokenizer.cls_token_id] + combined_tokens + [tokenizer.sep_token_id] + statement_tokens\n",
    "\n",
    "    # Truncate to max_input_length if necessary\n",
    "    input_tokens = input_tokens[:max_input_length - 1] + [tokenizer.sep_token_id]\n",
    "\n",
    "    # Convert token IDs back to text\n",
    "    tokenized_text = tokenizer.decode(input_tokens)\n",
    "\n",
    "    # Print tokenized output as text\n",
    "    print(\"Tokenized Text:\", tokenized_text)\n",
    "\n",
    "# Example usage\n",
    "debug_tokenization(\n",
    "    model_name=\"microsoft/deberta-v3-xsmall\",\n",
    "    primary_premise=\"Primary premise text here.\"*int(15),\n",
    "    secondary_premise=None,  # Use None if not applicable\n",
    "    statement=\"Statement text here.\",\n",
    "    premise_to_use=\"Original_truncated\",  # Or \"summarized_premise\"\n",
    "    max_premise_length=256,\n",
    "    max_input_length=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preprocessing\n",
    "class NliDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, premise_combined, primary_premise_to_use, secondary_premise_to_use, max_premise_length=256, max_input_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.premise_combined = premise_combined\n",
    "        self.primary_premise_to_use = primary_premise_to_use\n",
    "        self.secondary_premise_to_use = secondary_premise_to_use\n",
    "        self.max_premise_length = max_premise_length\n",
    "        self.max_input_length = max_input_length\n",
    "        self.keys = list(data.keys())  # Store the keys of the dictionary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]  # Use the index to get the corresponding key\n",
    "        item = self.data[key]  # Retrieve the item using the key\n",
    "\n",
    "        # Tokenize the statement and add [SEP] token at the beginning\n",
    "        statement_tokens = self.tokenizer.encode(item[\"Statement\"], add_special_tokens=False)\n",
    "        len_statement_tokens = len(statement_tokens)\n",
    "\n",
    "        remaining_for_premise_length = max(self.max_input_length - len_statement_tokens - 10, self.max_premise_length)\n",
    "\n",
    "        if self.premise_combined:\n",
    "            # Tokenize and use the summarized premise\n",
    "            combined_tokens = self.tokenizer.encode(item[self.primary_premise_to_use], add_special_tokens=False)\n",
    "        else:\n",
    "            # Tokenize the primary premise\n",
    "            primary_tokens = self.tokenizer.encode(item[self.primary_premise_to_use], add_special_tokens=False)\n",
    "            \n",
    "            individual_premise_length = int(remaining_for_premise_length) if item[\"Type\"] != \"Comparison\" else int(remaining_for_premise_length / 2)\n",
    "            combined_tokens = primary_tokens[:individual_premise_length]\n",
    "\n",
    "            if item[\"Type\"] == \"Comparison\":\n",
    "                # Tokenize the secondary premise\n",
    "                secondary_tokens = self.tokenizer.encode(item[self.secondary_premise_to_use], add_special_tokens=False)\n",
    "                combined_tokens += secondary_tokens[:individual_premise_length]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Combine tokens for final input\n",
    "        input_tokens = [self.tokenizer.cls_token_id] + combined_tokens + [self.tokenizer.sep_token_id] + statement_tokens\n",
    "\n",
    "        # Truncate to max_input_length if necessary\n",
    "        input_tokens = input_tokens[:self.max_input_length - 1] + [self.tokenizer.sep_token_id]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        input_ids = torch.tensor(input_tokens).unsqueeze(0)\n",
    "        attention_mask = torch.tensor([1] * len(input_tokens)).unsqueeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.flatten(),\n",
    "            \"attention_mask\": attention_mask.flatten(),\n",
    "            \"labels\": torch.tensor(item[\"Label\"] == \"Entailment\", dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def preprocess_data(tokenizer, data, premise_combined, primary_premise_to_use, secondary_premise_to_use):\n",
    "    return NliDataset(data, tokenizer, premise_combined, primary_premise_to_use, secondary_premise_to_use)\n",
    "\n",
    "# 2. Model Setup\n",
    "def get_model(model_name, device='cpu'):\n",
    "    # model = AutoModel.from_pretrained(model_name, num_labels=2)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# 3. Training and Evaluation\n",
    "def train(model, train_loader, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def evaluate(model, data_loader, set_name, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=f\"Evaluating on {set_name}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(F.log_softmax(logits, dim=1), dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average=\"macro\", zero_division=0)\n",
    "    return accuracy, f1, predictions, true_labels\n",
    "\n",
    "def save_predictions(data, predictions, file_path):\n",
    "    # Assume predictions is a list of integers (0 or 1)\n",
    "    # Convert predictions to the corresponding label strings\n",
    "    label_predictions = [\"Entailment\" if pred == 1 else \"Contradiction\" for pred in predictions]\n",
    "\n",
    "    # Make a copy of the data to avoid modifying the original data\n",
    "    updated_data = {key: dict(value, prediction=label_predictions[i]) for i, (key, value) in enumerate(data.items())}\n",
    "\n",
    "    # Save the updated data with predictions to a file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(updated_data, file, indent=4)\n",
    "\n",
    "def save_model_and_optimizer(model, optimizer, file_path):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_premises_text(gold, raw_data_location):\n",
    "  for data in gold:\n",
    "        section = gold[data][\"Section_id\"]\n",
    "        primary_id = gold[data][\"Primary_id\"]\n",
    "        with open(f'{raw_data_location}/{primary_id}.json') as f:\n",
    "            primary = json.load(f)\n",
    "\n",
    "        primary_premise = ' '.join(primary[section])\n",
    "        gold[data][\"Primary_premise\"] = primary_premise\n",
    "\n",
    "        if gold[data][\"Type\"] == 'Comparison':\n",
    "            secondary_id = gold[data][\"Secondary_id\"]\n",
    "            with open(f'{raw_data_location}/{secondary_id}.json') as f:\n",
    "                secondary = json.load(f)\n",
    "            secondary_premise = ' '.join(secondary[section])\n",
    "            gold[data][\"Secondary_premise\"] = secondary_premise\n",
    "\n",
    "with open('data\\\\raw\\\\train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('data\\\\raw\\\\dev.json') as f:\n",
    "    dev_data = json.load(f)\n",
    "with open('data\\\\raw\\\\test.json') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "get_premises_text(train_data, 'data\\\\raw\\\\CT')\n",
    "get_premises_text(dev_data, 'data\\\\raw\\\\CT')\n",
    "get_premises_text(test_data, 'data\\\\raw\\\\CT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Zero-Shot Summarizations and Fine-Tuned Summarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_dict(new_data_dict, old_data_dict, new_key, key_to_use):\n",
    "    for key in new_data_dict:\n",
    "        if key_to_use in new_data_dict[key]:\n",
    "            old_data_dict[key][new_key] = new_data_dict[key][key_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fine_tuning_steps in [0, 2, 5, 7, 10]:\n",
    "    with open(f'data\\\\raw\\\\summary_train_{fine_tuning_steps}.json') as f:\n",
    "        train_data_summarized = json.load(f)\n",
    "\n",
    "    with open(f'data\\\\raw\\\\summary_dev_{fine_tuning_steps}.json') as f:\n",
    "        dev_data_summarized = json.load(f)\n",
    "\n",
    "    with open(f'data\\\\raw\\\\summary_test_{fine_tuning_steps}.json') as f:\n",
    "        test_data_summarized = json.load(f)\n",
    "\n",
    "    update_data_dict(train_data_summarized, train_data, f\"Summarized_Primary_premise_{fine_tuning_steps}\", \"Primary_Premise\")\n",
    "    update_data_dict(dev_data_summarized, dev_data, f\"Summarized_Primary_premise_{fine_tuning_steps}\", \"Primary_Premise\")\n",
    "    update_data_dict(test_data_summarized, test_data, f\"Summarized_Primary_premise_{fine_tuning_steps}\", \"Primary_Premise\")\n",
    "\n",
    "    update_data_dict(train_data_summarized, train_data, f\"Summarized_Secondary_premise_{fine_tuning_steps}\", \"Secondary_Premise\")\n",
    "    update_data_dict(dev_data_summarized, dev_data, f\"Summarized_Secondary_premise_{fine_tuning_steps}\", \"Secondary_Premise\")\n",
    "    update_data_dict(test_data_summarized, test_data, f\"Summarized_Secondary_premise_{fine_tuning_steps}\", \"Secondary_Premise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fine_tuning_steps in [0, 2, 5, 7]:\n",
    "    with open(f'data\\\\raw\\\\scifive_train_{fine_tuning_steps}.json') as f:\n",
    "        train_data_summarized = json.load(f)\n",
    "\n",
    "    with open(f'data\\\\raw\\\\scifive_dev_{fine_tuning_steps}.json') as f:\n",
    "        dev_data_summarized = json.load(f)\n",
    "\n",
    "    with open(f'data\\\\raw\\\\scifive_test_{fine_tuning_steps}.json') as f:\n",
    "        test_data_summarized = json.load(f)\n",
    "\n",
    "    update_data_dict(train_data_summarized, train_data, f\"Scifive_Primary_premise_{fine_tuning_steps}\", \"Primary_Premise\")\n",
    "    update_data_dict(dev_data_summarized, dev_data, f\"Scifive_Primary_premise_{fine_tuning_steps}\", \"Primary_Premise\")\n",
    "    update_data_dict(test_data_summarized, test_data, f\"Scifive_Primary_premise_{fine_tuning_steps}\", \"Primary_Premise\")\n",
    "\n",
    "    update_data_dict(train_data_summarized, train_data, f\"Scifive_Secondary_premise_{fine_tuning_steps}\", \"Secondary_Premise\")\n",
    "    update_data_dict(dev_data_summarized, dev_data, f\"Scifive_Seconday_premise_{fine_tuning_steps}\", \"Secondary_Premise\")\n",
    "    update_data_dict(test_data_summarized, test_data, f\"Scifive_Seconday_premise_{fine_tuning_steps}\", \"Secondary_Premise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fine_tuning_steps in [0]:\n",
    "    with open(f'data\\\\raw\\\\combined_train_{fine_tuning_steps}.json') as f:\n",
    "        train_data_summarized = json.load(f)\n",
    "\n",
    "    with open(f'data\\\\raw\\\\combined_dev_{fine_tuning_steps}.json') as f:\n",
    "        dev_data_summarized = json.load(f)\n",
    "\n",
    "    with open(f'data\\\\raw\\\\combined_test_{fine_tuning_steps}.json') as f:\n",
    "        test_data_summarized = json.load(f)\n",
    "\n",
    "    update_data_dict(train_data_summarized, train_data, f\"Combined_Primary_premise_{fine_tuning_steps}\", \"Primary_Premise\")\n",
    "    update_data_dict(dev_data_summarized, dev_data, f\"Combined_Primary_premise_{fine_tuning_steps}\", \"Primary_Premise\")\n",
    "    update_data_dict(test_data_summarized, test_data, f\"Combined_Primary_premise_{fine_tuning_steps}\", \"Primary_Premise\")\n",
    "\n",
    "    # update_data_dict(train_data_summarized, train_data, f\"Summarized_Secondary_premise_{fine_tuning_steps}\", \"Secondary_Premise\")\n",
    "    # update_data_dict(dev_data_summarized, dev_data, f\"Summarized_Secondary_premise_{fine_tuning_steps}\", \"Secondary_Premise\")\n",
    "    # update_data_dict(test_data_summarized, test_data, f\"Summarized_Secondary_premise_{fine_tuning_steps}\", \"Secondary_Premise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Type': 'Comparison',\n",
       " 'Section_id': 'Intervention',\n",
       " 'Primary_id': 'NCT01928186',\n",
       " 'Secondary_id': 'NCT00684983',\n",
       " 'Statement': 'All the primary trial participants do not receive any oral capecitabine, oral lapatinib ditosylate or cixutumumab IV, in conrast all the secondary trial subjects receive these.',\n",
       " 'Label': 'Contradiction',\n",
       " 'Primary_premise': 'INTERVENTION 1:    Diagnostic (FLT PET)   Patients with early stage, ER positive primary breast cancer undergo FLT PET scan at baseline and 1-6 weeks after the start of standard endocrine treatment. The surgery follows 1-7 days after the second FLT PET scan.   Tracer used in the FLT PET (positron emission tomography) scanning procedure: [F18] fluorothymidine.   Positron Emission Tomography: Undergo FLT PET   Laboratory Biomarker Analysis: Correlative studies - Ki67 staining of the tumor tissue in the biopsy and surgical specimen.',\n",
       " 'Secondary_premise': 'INTERVENTION 1:    Arm A   Patients receive oral capecitabine twice daily on days 1-14 and oral lapatinib ditosylate once daily on days 1-21. Courses repeat every 21 days in the absence of disease progression or unacceptable toxicity. lapatinib ditosylate: Given PO and capecitabine: Given PO INTERVENTION 2:    Arm B   Patients receive capecitabine and lapatinib ditosylate as in arm I. Patients also receive cixutumumab IV over 1-1½ hours on days 1, 8, and 15. Courses repeat every 21 days in the absence of disease progression or unacceptable toxicity. cixutumumab: Given IV, lapatinib ditosylate: Given PO and capecitabine: Given PO',\n",
       " 'Summarized_Primary_premise_0': 'summary: INTERVENTION 1: Diagnostic PET (FLT PET) Patients with early stage, ER positive primary breast cancer undergo FLT PET scan at baseline and 1-6 weeks after the start of standard endocrine treatment. The surgery follows 1-7 days after the second FLAT PET scan. Tracer used in the FLT  PET (positron emission tomography) scanning procedure: [F18] fluorothymidine. Positron Emission Tomography: Undergo FLT PE Laboratory Biomarker Analysis: Correlative studies - Ki67 staining of the tumor tissue in the biopsy and surgical specimen.',\n",
       " 'Summarized_Secondary_premise_0': 'INTERVENTION 2: Arm B Patients receive capecitabine and lapatinib ditosylate as in arm I. Courses repeat every 21 days in the absence of disease progression or unacceptable toxicity. cixutumumab IV over 1-112 hours on days 1, 8, and 15.',\n",
       " 'Summarized_Primary_premise_2': 'patients with early stage, ER positive primary breast cancer undergo FLT PET scan at baseline and 1-6 weeks after the start of standard endocrine treatment.',\n",
       " 'Summarized_Secondary_premise_2': 'patients receive oral capecitabine and lapatinib ditosylate as in arm I . courses repeat every 21 days in the absence of disease progression or unacceptable toxicity.',\n",
       " 'Summarized_Primary_premise_5': 'Primary breast cancer patients with early stage, ER positive primary breast cancer undergo FLT PET scan at baseline and 1-6 weeks after the start of standard endocrine treatment.',\n",
       " 'Summarized_Secondary_premise_5': 'Patients receive oral capecitabine twice daily on days 1-14 and oral lapatinib ditosylate once daily on day 1-21. Courses repeat every 21 days in the absence of disease progression or unacceptable toxicity.',\n",
       " 'Summarized_Primary_premise_7': 'Patients with early stage, ER positive primary breast cancer undergo FLT PET scan at baseline and 1-6 weeks after the start of standard endocrine treatment.',\n",
       " 'Summarized_Secondary_premise_7': 'Patients in the primary trial receive oral capecitabine twice daily on days 1-14 and oral lapatinib ditosylate once daily on day 1-21.',\n",
       " 'Summarized_Primary_premise_10': 'Patients with early stage, ER positive primary breast cancer undergo a FLT PET scan at baseline and 1-6 weeks after the start of standard endocrine treatment.',\n",
       " 'Summarized_Secondary_premise_10': 'Patients in the primary trial receive oral capecitabine twice daily on days 1-14 and oral lapatinib ditosylate once daily on day 1-21.',\n",
       " 'Scifive_Primary_premise_0': 'OUTCOME 2: OUTCOME 2: Therapeutic (FLT PET) 2: Therapeutic (FLT PET) 2: Therapeutic (FLT PET) - 2::',\n",
       " 'Scifive_Secondary_premise_0': 'PARTNER PARTNER PARTNER PARTNER PARTNER PARTNER 3: Arm A Patients receive capecitabine twice daily on days 1-14 and oral capecitabine twice daily on days 1-21. Courses repeat every 21 days in the absence of disease progression or unacceptable toxicity. capecitabine: Given PO and capecitabine: Given PO INTERVENTION 2: Arm IION 3: Arm A: Arm B: Arm A: Arm A oral capecitabine twice daily on days 1-14 and oral capecitabine twice daily on days 1-14. Patients also receive capecitabine PO. and capecitabine: Given PO INTERVENTION 3: Arm C PO INTERVENTION 3: Arm A and capecitabine: Given PO INTERVENTION 3: Arm C......,,, capecitabine: Given PO and PO: Given PO, and IN',\n",
       " 'Scifive_Primary_premise_2': 'randomized trial.',\n",
       " 'Scifive_Secondary_premise_2': 'randomized trial.',\n",
       " 'Scifive_Primary_premise_5': 'Patients in the primary trial receive a PET scan at baseline and 1 week after the start of standard endocrine treatment.',\n",
       " 'Scifive_Secondary_premise_5': 'Patients in the primary trial receive oral capecitabine twice daily on days 1-14 and oral lapatinib ditosylate once daily on days 1-21.',\n",
       " 'Scifive_Primary_premise_7': 'Patients in the primary trial receive a FLT PET scan at baseline and 1 week after the start of standard endocrine treatment.',\n",
       " 'Scifive_Secondary_premise_7': 'Patients in the primary trial receive oral capecitabine twice daily on days 1-14 and oral lapatinib ditosylate once daily on days 1-21.',\n",
       " 'Combined_Primary_premise_0': 'Follow the first FLT PET scan at baseline and 1-6 weeks after the start of standard endocrine treatment. Undergo a double-blind, double-assisted, randomized, placebo-controlled clinical trial.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[list(train_data.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(epochs, epochs_to_save, learning_rate, results_location, device, model_name):\n",
    "    os.makedirs(results_location, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = preprocess_data(tokenizer, train_data, premise_combined, primary_premise_to_use, secondary_premise_to_use)\n",
    "    dev_dataset = preprocess_data(tokenizer, dev_data, premise_combined, primary_premise_to_use, secondary_premise_to_use)\n",
    "    test_dataset = preprocess_data(tokenizer, test_data, premise_combined, primary_premise_to_use, secondary_premise_to_use)\n",
    "\n",
    "    # Initialize the data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "    # Create DataLoader with DataCollator\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "    model = get_model(model_name, device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    clasification_reports_dev, clasification_reports_test = {}, {}\n",
    "\n",
    "    best_test_predictions, best_test_f1 = None, 0\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        train(model, train_loader, optimizer, lr_scheduler, device)\n",
    "        if epoch in epochs_to_save:\n",
    "            train_accuracy, train_f1, train_predictions, dev_labels = evaluate(model, train_loader, \"Train\", device)\n",
    "            dev_accuracy, dev_f1, dev_predictions, dev_labels = evaluate(model, dev_loader, \"Dev\", device)\n",
    "            test_accuracy, test_f1, test_predictions, test_labels = evaluate(model, test_loader, \"Test\", device)\n",
    "\n",
    "            print(f\"Epoch {epoch} - Train Accuracy: {train_accuracy:0.5f}, \\t Train F1: {train_f1:0.5f}\")\n",
    "            print(f\"Epoch {epoch} - Dev   Accuracy: {dev_accuracy:0.5f}, \\t Dev   F1: {dev_f1:0.5f}\")\n",
    "            print(f\"Epoch {epoch} - Test  Accuracy: {test_accuracy:0.5f}, \\t Test  F1: {test_f1:0.5f}\")\n",
    "            clasification_reports_dev[epoch] = classification_report(dev_labels, dev_predictions, output_dict=True, target_names=['Contradiction', 'Entailment'], zero_division=0)\n",
    "            clasification_reports_test[epoch] = classification_report(test_labels, test_predictions, output_dict=True, target_names=['Contradiction', 'Entailment'], zero_division=0)\n",
    "\n",
    "            dev_save_path = os.path.join(results_location, f\"dev_predictions_epoch{epoch}_{premise_prefix}.json\")\n",
    "            test_save_path = os.path.join(results_location, f\"test_predictions_epoch{epoch}_{premise_prefix}.json\")\n",
    "            model_save_path = os.path.join(results_location, f\"model_optimizer_epoch{epoch}_{premise_prefix}.pt\")\n",
    "\n",
    "            save_predictions(dev_data, dev_predictions, dev_save_path)\n",
    "            save_predictions(test_data, test_predictions, test_save_path)\n",
    "            save_model_and_optimizer(model, optimizer, model_save_path)\n",
    "\n",
    "            if test_f1 > best_test_f1:\n",
    "                best_test_f1 = test_f1\n",
    "                best_test_predictions = test_predictions\n",
    "    return best_test_f1, best_test_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Helper function to prepare data for vectorization\n",
    "def prepare_data(data):\n",
    "    combined_texts = []\n",
    "    for key, value in data.items():\n",
    "        premise = value['Primary_premise']\n",
    "        if 'Secondary_premise' in value:\n",
    "            premise += ' ' + value['Secondary_premise']\n",
    "        combined_texts.append(premise)\n",
    "    return combined_texts\n",
    "\n",
    "# Prepare the train, dev, and test data\n",
    "train_texts = prepare_data(train_data)\n",
    "dev_texts = prepare_data(dev_data)\n",
    "test_texts = prepare_data(test_data)\n",
    "\n",
    "# Create and fit the TF-IDF vectorizer on the training data\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "# Helper function to summarize a text\n",
    "def summarize_text(text, vectorizer, max_words=256):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) == 1:\n",
    "        return sentences[0]\n",
    "\n",
    "    tfidf_matrix = vectorizer.transform(sentences)\n",
    "    sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
    "\n",
    "    num_sentences = int(len(sentences) * 0.3)\n",
    "    top_sentence_indices = np.argsort(sentence_scores)[-num_sentences:]\n",
    "    top_sentences = [sentences[i] for i in sorted(top_sentence_indices)]\n",
    "\n",
    "    summary = ' '.join(top_sentences)\n",
    "    summary_words = summary.split(' ')\n",
    "    if len(summary_words) > max_words:\n",
    "        summary = ' '.join(summary_words[:max_words])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Function to process and summarize data\n",
    "def process_and_summarize(data, vectorizer):\n",
    "    for key, value in data.items():\n",
    "        premise = value['Primary_premise']\n",
    "        if 'Secondary_premise' in value:\n",
    "            premise += ' ' + value['Secondary_premise']\n",
    "\n",
    "        summary = summarize_text(premise, vectorizer)\n",
    "        data[key]['Extractive_Primary_premise'] = summary\n",
    "\n",
    "# Process and summarize each dataset\n",
    "process_and_summarize(train_data, vectorizer)\n",
    "process_and_summarize(dev_data, vectorizer)\n",
    "process_and_summarize(test_data, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'test_final.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "experiments_f1_dict = {'Pred_T5_Combined_0': 0.612,\n",
    "                        'Pred_T5_Separate_0': 0.6368829049367606,\n",
    "                        'Pred_T5_Separate_2': 0.6311501699916607,\n",
    "                        'Pred_T5_Separate_5': 0.6152551339393064,\n",
    "                        'Pred_T5_Separate_7': 0.5177701692626706,\n",
    "                        'Pred_T5_Separate_10': 0.617961796179618,\n",
    "                        'Pred_Truncated': 0.503923103529883,\n",
    "                        'Pred_Extractive': 0.5458404074702886}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Type', 'Section_id', 'Primary_id', 'Secondary_id', 'Statement', 'Label', 'Primary_premise', 'Secondary_premise', 'Summarized_Primary_premise_0', 'Summarized_Secondary_premise_0', 'Summarized_Primary_premise_2', 'Summarized_Secondary_premise_2', 'Summarized_Primary_premise_5', 'Summarized_Secondary_premise_5', 'Summarized_Primary_premise_7', 'Summarized_Secondary_premise_7', 'Summarized_Primary_premise_10', 'Summarized_Secondary_premise_10', 'Scifive_Primary_premise_0', 'Scifive_Seconday_premise_0', 'Scifive_Primary_premise_2', 'Scifive_Seconday_premise_2', 'Scifive_Primary_premise_5', 'Scifive_Seconday_premise_5', 'Scifive_Primary_premise_7', 'Scifive_Seconday_premise_7', 'Combined_Primary_premise_0', 'Pred_T5_Combined_0', 'Pred_T5_Separate_0', 'Pred_T5_Separate_2', 'Pred_T5_Separate_5', 'Pred_T5_Separate_7', 'Pred_T5_Separate_10', 'Pred_Truncated', 'Extractive_Primary_premise', 'Pred_Extractive'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[list(test_data.keys())[0]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred_T5_Combined_0\n",
      "Pred_T5_Separate_0\n",
      "Pred_T5_Separate_2\n",
      "Pred_T5_Separate_5\n",
      "Pred_T5_Separate_7\n",
      "Pred_T5_Separate_10\n",
      "Pred_Truncated\n",
      "Pred_Extractive\n",
      "Pred_SciFive_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at cross-encoder/nli-deberta-v3-base and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epochs:   0%|          | 0/40 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Training: 100%|██████████| 107/107 [00:20<00:00,  5.30it/s]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.40it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.26it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 0 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 0 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.52it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.65it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.65it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 1 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 1 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.64it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.77it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.41it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 2 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 2 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.57it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.52it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.47it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 3 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 3 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.47it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.31it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.23it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 4 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 4 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.50it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.40it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.31it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 5 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 5 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.53it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.43it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.64it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 6 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 6 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.57it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.65it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.68it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 7 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 7 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.59it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.45it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.54it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 8 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 8 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.60it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.40it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.71it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 9 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 9 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.54it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.55it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.62it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 10 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 10 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.57it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.54it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.49it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train Accuracy: 0.50059, \t Train F1: 0.33464\n",
      "Epoch 11 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 11 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.55it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.53it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.54it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 12 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 12 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.63it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:07<00:00, 15.21it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.47it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train Accuracy: 0.50176, \t Train F1: 0.33930\n",
      "Epoch 13 - Dev   Accuracy: 0.50500, \t Dev   F1: 0.35275\n",
      "Epoch 13 - Test  Accuracy: 0.50600, \t Test  F1: 0.34991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.56it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.39it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.57it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 14.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train Accuracy: 0.50000, \t Train F1: 0.38543\n",
      "Epoch 14 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.35031\n",
      "Epoch 14 - Test  Accuracy: 0.50000, \t Test  F1: 0.38528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.49it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.95it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.12it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 15 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 15 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.77it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.90it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.98it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 16 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 16 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.82it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.15it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.20it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 17 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 17 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.75it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.93it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.29it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 18 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 18 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.82it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.05it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.20it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Train Accuracy: 0.50059, \t Train F1: 0.33568\n",
      "Epoch 19 - Dev   Accuracy: 0.50500, \t Dev   F1: 0.34435\n",
      "Epoch 19 - Test  Accuracy: 0.50000, \t Test  F1: 0.33686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.78it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.77it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.01it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 20 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 20 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.79it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.84it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.17it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 21 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 21 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.70it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.91it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.15it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Train Accuracy: 0.50412, \t Train F1: 0.44685\n",
      "Epoch 22 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.47207\n",
      "Epoch 22 - Test  Accuracy: 0.51400, \t Test  F1: 0.45132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.76it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.84it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.95it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 23 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 23 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.84it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.92it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.68it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 24 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 24 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.83it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.74it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.06it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 25 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 25 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.79it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.80it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.17it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 26 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 26 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.73it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.08it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.17it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 27 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 27 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.73it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.81it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.98it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 28 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 28 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.76it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.97it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.95it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 29 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 29 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.72it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.89it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.98it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 30 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 30 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.71it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.71it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.15it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 31 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 31 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.80it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.04it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.03it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 32 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 32 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.76it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.92it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.84it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 33 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 33 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.80it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.90it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.98it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Train Accuracy: 0.50176, \t Train F1: 0.33930\n",
      "Epoch 34 - Dev   Accuracy: 0.50500, \t Dev   F1: 0.36077\n",
      "Epoch 34 - Test  Accuracy: 0.50400, \t Test  F1: 0.34559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.65it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.93it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.95it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 35 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 35 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.73it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.80it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 19.06it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 36 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 36 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.73it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.95it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.98it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 37 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 37 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.55it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.55it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.65it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 38 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 38 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.54it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.63it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 18.65it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Train Accuracy: 0.50000, \t Train F1: 0.33333\n",
      "Epoch 39 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 39 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 40/40 [20:57<00:00, 31.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test F1: 0.45132\n",
      "Pred_SciFive_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahr\\anaconda3\\envs\\lm-prompt-turbo2\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at cross-encoder/nli-deberta-v3-base and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epochs:   0%|          | 0/40 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# experiments_f1_dict = {}\n",
    "\n",
    "for premise_prefix in [\"Combined_\", \"Summarized_\", \"\", \"Extractive_\", \"Scifive_\"]:\n",
    "    for fine_tuning_steps_suffix in [\"_0\", \"_2\", \"_5\", \"_7\", \"_10\", \"\"]:\n",
    "        if premise_prefix == \"Combined_\" and fine_tuning_steps_suffix != \"_0\":\n",
    "            continue\n",
    "        if premise_prefix == \"Scifive_\" and (fine_tuning_steps_suffix == \"_10\" or fine_tuning_steps_suffix == \"\"):\n",
    "            continue\n",
    "        if premise_prefix == \"\" and fine_tuning_steps_suffix != \"\":\n",
    "            continue\n",
    "        if premise_prefix == \"Summarized_\" and fine_tuning_steps_suffix == \"\":\n",
    "            continue\n",
    "        if premise_prefix == \"Extractive_\" and fine_tuning_steps_suffix != \"\":\n",
    "            continue\n",
    "        \n",
    "        premise_combined = premise_prefix == \"Combined_\" or premise_prefix == \"Extractive_\" or premise_prefix == \"Scifive_\"\n",
    "        label_name = \"Pred_\"\n",
    "        if premise_prefix == \"Combined_\":\n",
    "            label_name += f\"T5_Combined{fine_tuning_steps_suffix}\" \n",
    "        elif premise_prefix == \"Scifive_\":\n",
    "            label_name += f\"SciFive{fine_tuning_steps_suffix}\"\n",
    "        elif premise_prefix == \"Summarized_\":\n",
    "            label_name += f\"T5_Separate{fine_tuning_steps_suffix}\"\n",
    "        elif premise_prefix == \"Extractive_\":\n",
    "            label_name += f\"Extractive\"\n",
    "        else:\n",
    "            label_name += f\"Truncated\"\n",
    "\n",
    "        print(label_name)\n",
    "\n",
    "        if label_name in experiments_f1_dict:\n",
    "            continue\n",
    "\n",
    "        primary_premise_to_use = premise_prefix + \"Primary_premise\" + fine_tuning_steps_suffix\n",
    "        secondary_premise_to_use = premise_prefix + \"Secondary_premise\" + fine_tuning_steps_suffix\n",
    "\n",
    "        epochs = 40 #if not(premise_combined) else 20\n",
    "        epochs_to_save = np.arange(epochs)\n",
    "        learning_rate = 4e-5 if not(premise_combined) else 5e-5\n",
    "        batch_size = 16 if premise_prefix != \"Scifive_\" else 16\n",
    "        if \"Truncated\" in label_name:\n",
    "            batch_size = 12\n",
    "            print(f\"batch_size: {batch_size}\")\n",
    "            learning_rate = 5e-5\n",
    "        if premise_prefix == \"Summarized_\":\n",
    "            learning_rate = 5e-5\n",
    "        results_location = \"results\"  # Replace with your location\n",
    "        device = DEVICE\n",
    "        model_name = MODEL_NAME\n",
    "\n",
    "        best_test_f1, best_test_predictions = train_eval_model(epochs, epochs_to_save, learning_rate, results_location, device, model_name)\n",
    "\n",
    "        experiments_f1_dict[label_name] = best_test_f1\n",
    "\n",
    "        print(f\"Best Test F1: {best_test_f1:0.5f}\")\n",
    "        for key_idx, key in enumerate(test_data):\n",
    "            test_data[key][label_name] = \"Entailment\" if best_test_predictions[key_idx] else \"Contradiction\"\n",
    "        \n",
    "        with open(f'test_final.json', 'w') as f:\n",
    "            json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pred_T5_Combined_0': 0.612,\n",
       " 'Pred_T5_Separate_0': 0.6368829049367606,\n",
       " 'Pred_T5_Separate_2': 0.6311501699916607,\n",
       " 'Pred_T5_Separate_5': 0.6152551339393064,\n",
       " 'Pred_T5_Separate_7': 0.5177701692626706,\n",
       " 'Pred_T5_Separate_10': 0.617961796179618,\n",
       " 'Pred_Truncated': 0.503923103529883,\n",
       " 'Pred_Extractive': 0.5458404074702886}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_f1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_steps_suffix = \"_0\"  # \"_0\" or \"_2\" or \"_5\" or \"_7\" or \"_10\" or \"\"\n",
    "premise_prefix = \"Combined_\" # \"Summarized_\" or \"\"Scifive_\" or \"\" or \"Combined_\"\n",
    "premise_combined = premise_prefix == \"Combined_\"\n",
    "\n",
    "epochs = 40\n",
    "epochs_to_save = np.arange(epochs)\n",
    "batch_size = 16\n",
    "learning_rate = 4e-5\n",
    "results_location = \"results\"  # Replace with your location\n",
    "device = DEVICE\n",
    "model_name = MODEL_NAME\n",
    "\n",
    "primary_premise_to_use = premise_prefix + \"Primary_premise\" + fine_tuning_steps_suffix\n",
    "secondary_premise_to_use = premise_prefix + \"Secondary_premise\" + fine_tuning_steps_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahr\\anaconda3\\envs\\lm-prompt-turbo2\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at cross-encoder/nli-deberta-v3-base and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(results_location, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_dataset = preprocess_data(tokenizer, train_data, premise_combined, primary_premise_to_use, secondary_premise_to_use)\n",
    "dev_dataset = preprocess_data(tokenizer, dev_data, premise_combined, primary_premise_to_use, secondary_premise_to_use)\n",
    "test_dataset = preprocess_data(tokenizer, test_data, premise_combined, primary_premise_to_use, secondary_premise_to_use)\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# Create DataLoader with DataCollator\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "model = get_model(model_name, device)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-encoder/nli-deberta-v3-base\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/40 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.36it/s]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.30it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.15it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Accuracy: 0.53471, \t Train F1: 0.51705\n",
      "Epoch 0 - Dev   Accuracy: 0.54000, \t Dev   F1: 0.51431\n",
      "Epoch 0 - Test  Accuracy: 0.55200, \t Test  F1: 0.53092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.58it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.38it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.41it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Accuracy: 0.50412, \t Train F1: 0.34644\n",
      "Epoch 1 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 1 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.56it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.47it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.09it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Accuracy: 0.50059, \t Train F1: 0.33464\n",
      "Epoch 2 - Dev   Accuracy: 0.50000, \t Dev   F1: 0.33333\n",
      "Epoch 2 - Test  Accuracy: 0.50000, \t Test  F1: 0.33333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.57it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.54it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.27it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Accuracy: 0.56588, \t Train F1: 0.52107\n",
      "Epoch 3 - Dev   Accuracy: 0.56000, \t Dev   F1: 0.50623\n",
      "Epoch 3 - Test  Accuracy: 0.53600, \t Test  F1: 0.48596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.54it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:07<00:00, 15.24it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.29it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Accuracy: 0.62294, \t Train F1: 0.62002\n",
      "Epoch 4 - Dev   Accuracy: 0.60500, \t Dev   F1: 0.60060\n",
      "Epoch 4 - Test  Accuracy: 0.61200, \t Test  F1: 0.60742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.56it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.70it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.23it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Accuracy: 0.65471, \t Train F1: 0.64810\n",
      "Epoch 5 - Dev   Accuracy: 0.65000, \t Dev   F1: 0.64489\n",
      "Epoch 5 - Test  Accuracy: 0.59200, \t Test  F1: 0.58910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.57it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.33it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.07it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Accuracy: 0.66235, \t Train F1: 0.63691\n",
      "Epoch 6 - Dev   Accuracy: 0.60000, \t Dev   F1: 0.56597\n",
      "Epoch 6 - Test  Accuracy: 0.58600, \t Test  F1: 0.53811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.61it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.50it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.48it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Accuracy: 0.73941, \t Train F1: 0.73636\n",
      "Epoch 7 - Dev   Accuracy: 0.55500, \t Dev   F1: 0.53923\n",
      "Epoch 7 - Test  Accuracy: 0.57200, \t Test  F1: 0.56487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.47it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.45it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 15.99it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Accuracy: 0.74706, \t Train F1: 0.73859\n",
      "Epoch 8 - Dev   Accuracy: 0.63000, \t Dev   F1: 0.61118\n",
      "Epoch 8 - Test  Accuracy: 0.58200, \t Test  F1: 0.55219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.60it/s]/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 15.35it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.11it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Accuracy: 0.84176, \t Train F1: 0.84176\n",
      "Epoch 9 - Dev   Accuracy: 0.65500, \t Dev   F1: 0.65492\n",
      "Epoch 9 - Test  Accuracy: 0.59600, \t Test  F1: 0.59313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.82it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.12it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.04it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Accuracy: 0.86294, \t Train F1: 0.86291\n",
      "Epoch 10 - Dev   Accuracy: 0.63000, \t Dev   F1: 0.62996\n",
      "Epoch 10 - Test  Accuracy: 0.62200, \t Test  F1: 0.62014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:19<00:00,  5.63it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:07<00:00, 15.04it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 15.85it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:02<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train Accuracy: 0.87529, \t Train F1: 0.87504\n",
      "Epoch 11 - Dev   Accuracy: 0.62000, \t Dev   F1: 0.61966\n",
      "Epoch 11 - Test  Accuracy: 0.60400, \t Test  F1: 0.59782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.92it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.37it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.08it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train Accuracy: 0.90000, \t Train F1: 0.90000\n",
      "Epoch 12 - Dev   Accuracy: 0.63000, \t Dev   F1: 0.62996\n",
      "Epoch 12 - Test  Accuracy: 0.61400, \t Test  F1: 0.61231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.93it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.30it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.11it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train Accuracy: 0.92000, \t Train F1: 0.91979\n",
      "Epoch 13 - Dev   Accuracy: 0.61500, \t Dev   F1: 0.61071\n",
      "Epoch 13 - Test  Accuracy: 0.60000, \t Test  F1: 0.59742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.06it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.75it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.69it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train Accuracy: 0.94706, \t Train F1: 0.94706\n",
      "Epoch 14 - Dev   Accuracy: 0.62000, \t Dev   F1: 0.62000\n",
      "Epoch 14 - Test  Accuracy: 0.59400, \t Test  F1: 0.59263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.04it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.92it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.57it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train Accuracy: 0.94353, \t Train F1: 0.94352\n",
      "Epoch 15 - Dev   Accuracy: 0.58500, \t Dev   F1: 0.58449\n",
      "Epoch 15 - Test  Accuracy: 0.59400, \t Test  F1: 0.59068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.06it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.78it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.71it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Train Accuracy: 0.95588, \t Train F1: 0.95588\n",
      "Epoch 16 - Dev   Accuracy: 0.58000, \t Dev   F1: 0.57895\n",
      "Epoch 16 - Test  Accuracy: 0.59800, \t Test  F1: 0.59624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.13it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.76it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.66it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Train Accuracy: 0.95824, \t Train F1: 0.95823\n",
      "Epoch 17 - Dev   Accuracy: 0.63500, \t Dev   F1: 0.63499\n",
      "Epoch 17 - Test  Accuracy: 0.58200, \t Test  F1: 0.57994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.11it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.78it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.64it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Train Accuracy: 0.96824, \t Train F1: 0.96823\n",
      "Epoch 18 - Dev   Accuracy: 0.61000, \t Dev   F1: 0.60984\n",
      "Epoch 18 - Test  Accuracy: 0.60200, \t Test  F1: 0.60100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.05it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.63it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.59it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Train Accuracy: 0.96824, \t Train F1: 0.96822\n",
      "Epoch 19 - Dev   Accuracy: 0.61000, \t Dev   F1: 0.60606\n",
      "Epoch 19 - Test  Accuracy: 0.61000, \t Test  F1: 0.59895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.09it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.19it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.29it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Train Accuracy: 0.97824, \t Train F1: 0.97823\n",
      "Epoch 20 - Dev   Accuracy: 0.62500, \t Dev   F1: 0.62492\n",
      "Epoch 20 - Test  Accuracy: 0.61400, \t Test  F1: 0.61344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  5.96it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.42it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.97it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Train Accuracy: 0.97706, \t Train F1: 0.97705\n",
      "Epoch 21 - Dev   Accuracy: 0.64500, \t Dev   F1: 0.64492\n",
      "Epoch 21 - Test  Accuracy: 0.60400, \t Test  F1: 0.60194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.91it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.03it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 16.37it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Train Accuracy: 0.98294, \t Train F1: 0.98294\n",
      "Epoch 22 - Dev   Accuracy: 0.64500, \t Dev   F1: 0.64299\n",
      "Epoch 22 - Test  Accuracy: 0.59600, \t Test  F1: 0.59584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.90it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.44it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.13it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Train Accuracy: 0.98471, \t Train F1: 0.98471\n",
      "Epoch 23 - Dev   Accuracy: 0.61500, \t Dev   F1: 0.61491\n",
      "Epoch 23 - Test  Accuracy: 0.60400, \t Test  F1: 0.60119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.92it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.65it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.15it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Train Accuracy: 0.98412, \t Train F1: 0.98412\n",
      "Epoch 24 - Dev   Accuracy: 0.58000, \t Dev   F1: 0.57962\n",
      "Epoch 24 - Test  Accuracy: 0.62800, \t Test  F1: 0.62482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.94it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.22it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.17it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Train Accuracy: 0.98765, \t Train F1: 0.98765\n",
      "Epoch 25 - Dev   Accuracy: 0.59500, \t Dev   F1: 0.59450\n",
      "Epoch 25 - Test  Accuracy: 0.62200, \t Test  F1: 0.62145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.91it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.63it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.15it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Train Accuracy: 0.98529, \t Train F1: 0.98529\n",
      "Epoch 26 - Dev   Accuracy: 0.61000, \t Dev   F1: 0.60996\n",
      "Epoch 26 - Test  Accuracy: 0.62600, \t Test  F1: 0.62456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:18<00:00,  5.86it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.48it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.15it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 16.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Train Accuracy: 0.98882, \t Train F1: 0.98882\n",
      "Epoch 27 - Dev   Accuracy: 0.60500, \t Dev   F1: 0.60452\n",
      "Epoch 27 - Test  Accuracy: 0.62400, \t Test  F1: 0.62158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  5.98it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.88it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.45it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Train Accuracy: 0.98765, \t Train F1: 0.98765\n",
      "Epoch 28 - Dev   Accuracy: 0.60000, \t Dev   F1: 0.59900\n",
      "Epoch 28 - Test  Accuracy: 0.61400, \t Test  F1: 0.61303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.13it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.42it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.57it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Train Accuracy: 0.98941, \t Train F1: 0.98941\n",
      "Epoch 29 - Dev   Accuracy: 0.63500, \t Dev   F1: 0.63294\n",
      "Epoch 29 - Test  Accuracy: 0.61400, \t Test  F1: 0.61387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.04it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.43it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.45it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Train Accuracy: 0.98765, \t Train F1: 0.98765\n",
      "Epoch 30 - Dev   Accuracy: 0.61000, \t Dev   F1: 0.60902\n",
      "Epoch 30 - Test  Accuracy: 0.60800, \t Test  F1: 0.60147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.11it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.78it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.57it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Train Accuracy: 0.99118, \t Train F1: 0.99118\n",
      "Epoch 31 - Dev   Accuracy: 0.58500, \t Dev   F1: 0.58474\n",
      "Epoch 31 - Test  Accuracy: 0.62000, \t Test  F1: 0.61802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.07it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.69it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.69it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Train Accuracy: 0.99059, \t Train F1: 0.99059\n",
      "Epoch 32 - Dev   Accuracy: 0.60000, \t Dev   F1: 0.59964\n",
      "Epoch 32 - Test  Accuracy: 0.61400, \t Test  F1: 0.61270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.10it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.75it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.66it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Train Accuracy: 0.99176, \t Train F1: 0.99176\n",
      "Epoch 33 - Dev   Accuracy: 0.60000, \t Dev   F1: 0.59964\n",
      "Epoch 33 - Test  Accuracy: 0.61600, \t Test  F1: 0.61461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.11it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.67it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.50it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Train Accuracy: 0.99235, \t Train F1: 0.99235\n",
      "Epoch 34 - Dev   Accuracy: 0.61000, \t Dev   F1: 0.60902\n",
      "Epoch 34 - Test  Accuracy: 0.61800, \t Test  F1: 0.61732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.23it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.71it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.57it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Train Accuracy: 0.99118, \t Train F1: 0.99118\n",
      "Epoch 35 - Dev   Accuracy: 0.60000, \t Dev   F1: 0.59936\n",
      "Epoch 35 - Test  Accuracy: 0.62000, \t Test  F1: 0.61880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  5.98it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.72it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.66it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Train Accuracy: 0.99176, \t Train F1: 0.99176\n",
      "Epoch 36 - Dev   Accuracy: 0.60500, \t Dev   F1: 0.60420\n",
      "Epoch 36 - Test  Accuracy: 0.61600, \t Test  F1: 0.61442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  5.96it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.90it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.62it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Train Accuracy: 0.99118, \t Train F1: 0.99118\n",
      "Epoch 37 - Dev   Accuracy: 0.60500, \t Dev   F1: 0.60380\n",
      "Epoch 37 - Test  Accuracy: 0.61800, \t Test  F1: 0.61745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  5.99it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.73it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.50it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Train Accuracy: 0.99176, \t Train F1: 0.99176\n",
      "Epoch 38 - Dev   Accuracy: 0.60500, \t Dev   F1: 0.60380\n",
      "Epoch 38 - Test  Accuracy: 0.61600, \t Test  F1: 0.61526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 107/107 [00:17<00:00,  6.08it/s]s/it]\n",
      "Evaluating on Train: 100%|██████████| 107/107 [00:06<00:00, 16.72it/s]\n",
      "Evaluating on Dev: 100%|██████████| 13/13 [00:00<00:00, 17.76it/s]\n",
      "Evaluating on Test: 100%|██████████| 32/32 [00:01<00:00, 17.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Train Accuracy: 0.99176, \t Train F1: 0.99176\n",
      "Epoch 39 - Dev   Accuracy: 0.60000, \t Dev   F1: 0.59855\n",
      "Epoch 39 - Test  Accuracy: 0.61800, \t Test  F1: 0.61732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 40/40 [20:17<00:00, 30.44s/it]\n"
     ]
    }
   ],
   "source": [
    "num_training_steps = epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "clasification_reports_dev, clasification_reports_test = {}, {}\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "    train(model, train_loader, optimizer, lr_scheduler, device)\n",
    "    if epoch in epochs_to_save:\n",
    "        train_accuracy, train_f1, train_predictions, dev_labels = evaluate(model, train_loader, \"Train\", device)\n",
    "        dev_accuracy, dev_f1, dev_predictions, dev_labels = evaluate(model, dev_loader, \"Dev\", device)\n",
    "        test_accuracy, test_f1, test_predictions, test_labels = evaluate(model, test_loader, \"Test\", device)\n",
    "\n",
    "        print(f\"Epoch {epoch} - Train Accuracy: {train_accuracy:0.5f}, \\t Train F1: {train_f1:0.5f}\")\n",
    "        print(f\"Epoch {epoch} - Dev   Accuracy: {dev_accuracy:0.5f}, \\t Dev   F1: {dev_f1:0.5f}\")\n",
    "        print(f\"Epoch {epoch} - Test  Accuracy: {test_accuracy:0.5f}, \\t Test  F1: {test_f1:0.5f}\")\n",
    "        clasification_reports_dev[epoch] = classification_report(dev_labels, dev_predictions, output_dict=True, target_names=['Contradiction', 'Entailment'], zero_division=0)\n",
    "        clasification_reports_test[epoch] = classification_report(test_labels, test_predictions, output_dict=True, target_names=['Contradiction', 'Entailment'], zero_division=0)\n",
    "\n",
    "        dev_save_path = os.path.join(results_location, f\"dev_predictions_epoch{epoch}_{premise_prefix}.json\")\n",
    "        test_save_path = os.path.join(results_location, f\"test_predictions_epoch{epoch}_{premise_prefix}.json\")\n",
    "        model_save_path = os.path.join(results_location, f\"model_optimizer_epoch{epoch}_{premise_prefix}.pt\")\n",
    "\n",
    "        save_predictions(dev_data, dev_predictions, dev_save_path)\n",
    "        save_predictions(test_data, test_predictions, test_save_path)\n",
    "        save_model_and_optimizer(model, optimizer, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Reports for Test Set\n",
      "{'Contradiction': {'precision': 0.5365168539325843, 'recall': 0.764, 'f1-score': 0.6303630363036304, 'support': 250}, 'Entailment': {'precision': 0.5902777777777778, 'recall': 0.34, 'f1-score': 0.43147208121827413, 'support': 250}, 'accuracy': 0.552, 'macro avg': {'precision': 0.563397315855181, 'recall': 0.552, 'f1-score': 0.5309175587609523, 'support': 500}, 'weighted avg': {'precision': 0.563397315855181, 'recall': 0.552, 'f1-score': 0.5309175587609523, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 250}, 'Entailment': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 250}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 500}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 250}, 'Entailment': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 250}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 500}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5221674876847291, 'recall': 0.848, 'f1-score': 0.6463414634146343, 'support': 250}, 'Entailment': {'precision': 0.5957446808510638, 'recall': 0.224, 'f1-score': 0.3255813953488372, 'support': 250}, 'accuracy': 0.536, 'macro avg': {'precision': 0.5589560842678964, 'recall': 0.536, 'f1-score': 0.4859614293817357, 'support': 500}, 'weighted avg': {'precision': 0.5589560842678963, 'recall': 0.536, 'f1-score': 0.48596142938173575, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5921052631578947, 'recall': 0.72, 'f1-score': 0.6498194945848375, 'support': 250}, 'Entailment': {'precision': 0.6428571428571429, 'recall': 0.504, 'f1-score': 0.5650224215246638, 'support': 250}, 'accuracy': 0.612, 'macro avg': {'precision': 0.6174812030075187, 'recall': 0.612, 'f1-score': 0.6074209580547506, 'support': 500}, 'weighted avg': {'precision': 0.6174812030075189, 'recall': 0.612, 'f1-score': 0.6074209580547506, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6105769230769231, 'recall': 0.508, 'f1-score': 0.5545851528384279, 'support': 250}, 'Entailment': {'precision': 0.5787671232876712, 'recall': 0.676, 'f1-score': 0.6236162361623616, 'support': 250}, 'accuracy': 0.592, 'macro avg': {'precision': 0.5946720231822972, 'recall': 0.5920000000000001, 'f1-score': 0.5891006945003947, 'support': 500}, 'weighted avg': {'precision': 0.5946720231822972, 'recall': 0.592, 'f1-score': 0.5891006945003947, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5523114355231143, 'recall': 0.908, 'f1-score': 0.686838124054463, 'support': 250}, 'Entailment': {'precision': 0.7415730337078652, 'recall': 0.264, 'f1-score': 0.3893805309734513, 'support': 250}, 'accuracy': 0.586, 'macro avg': {'precision': 0.6469422346154898, 'recall': 0.5860000000000001, 'f1-score': 0.5381093275139571, 'support': 500}, 'weighted avg': {'precision': 0.6469422346154897, 'recall': 0.586, 'f1-score': 0.5381093275139571, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5967741935483871, 'recall': 0.444, 'f1-score': 0.5091743119266054, 'support': 250}, 'Entailment': {'precision': 0.5573248407643312, 'recall': 0.7, 'f1-score': 0.6205673758865248, 'support': 250}, 'accuracy': 0.572, 'macro avg': {'precision': 0.5770495171563592, 'recall': 0.572, 'f1-score': 0.5648708439065651, 'support': 500}, 'weighted avg': {'precision': 0.5770495171563591, 'recall': 0.572, 'f1-score': 0.5648708439065652, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.554089709762533, 'recall': 0.84, 'f1-score': 0.6677265500794913, 'support': 250}, 'Entailment': {'precision': 0.6694214876033058, 'recall': 0.324, 'f1-score': 0.43665768194070076, 'support': 250}, 'accuracy': 0.582, 'macro avg': {'precision': 0.6117555986829194, 'recall': 0.582, 'f1-score': 0.552192116010096, 'support': 500}, 'weighted avg': {'precision': 0.6117555986829194, 'recall': 0.582, 'f1-score': 0.552192116010096, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5821917808219178, 'recall': 0.68, 'f1-score': 0.6273062730627307, 'support': 250}, 'Entailment': {'precision': 0.6153846153846154, 'recall': 0.512, 'f1-score': 0.5589519650655022, 'support': 250}, 'accuracy': 0.596, 'macro avg': {'precision': 0.5987881981032666, 'recall': 0.5960000000000001, 'f1-score': 0.5931291190641164, 'support': 500}, 'weighted avg': {'precision': 0.5987881981032668, 'recall': 0.596, 'f1-score': 0.5931291190641164, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6070175438596491, 'recall': 0.692, 'f1-score': 0.6467289719626169, 'support': 250}, 'Entailment': {'precision': 0.641860465116279, 'recall': 0.552, 'f1-score': 0.5935483870967742, 'support': 250}, 'accuracy': 0.622, 'macro avg': {'precision': 0.6244390044879641, 'recall': 0.622, 'f1-score': 0.6201386795296955, 'support': 500}, 'weighted avg': {'precision': 0.624439004487964, 'recall': 0.622, 'f1-score': 0.6201386795296956, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5833333333333334, 'recall': 0.728, 'f1-score': 0.6476868327402137, 'support': 250}, 'Entailment': {'precision': 0.6382978723404256, 'recall': 0.48, 'f1-score': 0.547945205479452, 'support': 250}, 'accuracy': 0.604, 'macro avg': {'precision': 0.6108156028368794, 'recall': 0.604, 'f1-score': 0.5978160191098328, 'support': 500}, 'weighted avg': {'precision': 0.6108156028368794, 'recall': 0.604, 'f1-score': 0.5978160191098328, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6007067137809188, 'recall': 0.68, 'f1-score': 0.6378986866791746, 'support': 250}, 'Entailment': {'precision': 0.631336405529954, 'recall': 0.548, 'f1-score': 0.5867237687366167, 'support': 250}, 'accuracy': 0.614, 'macro avg': {'precision': 0.6160215596554364, 'recall': 0.6140000000000001, 'f1-score': 0.6123112277078957, 'support': 500}, 'weighted avg': {'precision': 0.6160215596554364, 'recall': 0.614, 'f1-score': 0.6123112277078956, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6190476190476191, 'recall': 0.52, 'f1-score': 0.5652173913043478, 'support': 250}, 'Entailment': {'precision': 0.5862068965517241, 'recall': 0.68, 'f1-score': 0.6296296296296295, 'support': 250}, 'accuracy': 0.6, 'macro avg': {'precision': 0.6026272577996716, 'recall': 0.6000000000000001, 'f1-score': 0.5974235104669887, 'support': 500}, 'weighted avg': {'precision': 0.6026272577996715, 'recall': 0.6, 'f1-score': 0.5974235104669886, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5842293906810035, 'recall': 0.652, 'f1-score': 0.6162570888468809, 'support': 250}, 'Entailment': {'precision': 0.6063348416289592, 'recall': 0.536, 'f1-score': 0.5690021231422504, 'support': 250}, 'accuracy': 0.594, 'macro avg': {'precision': 0.5952821161549814, 'recall': 0.5940000000000001, 'f1-score': 0.5926296059945657, 'support': 500}, 'weighted avg': {'precision': 0.5952821161549814, 'recall': 0.594, 'f1-score': 0.5926296059945657, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5796610169491525, 'recall': 0.684, 'f1-score': 0.6275229357798164, 'support': 250}, 'Entailment': {'precision': 0.6146341463414634, 'recall': 0.504, 'f1-score': 0.5538461538461539, 'support': 250}, 'accuracy': 0.594, 'macro avg': {'precision': 0.597147581645308, 'recall': 0.5940000000000001, 'f1-score': 0.5906845448129852, 'support': 500}, 'weighted avg': {'precision': 0.597147581645308, 'recall': 0.594, 'f1-score': 0.5906845448129853, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5865724381625441, 'recall': 0.664, 'f1-score': 0.6228893058161351, 'support': 250}, 'Entailment': {'precision': 0.6129032258064516, 'recall': 0.532, 'f1-score': 0.5695931477516061, 'support': 250}, 'accuracy': 0.598, 'macro avg': {'precision': 0.5997378319844979, 'recall': 0.5980000000000001, 'f1-score': 0.5962412267838706, 'support': 500}, 'weighted avg': {'precision': 0.5997378319844979, 'recall': 0.598, 'f1-score': 0.5962412267838706, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5719298245614035, 'recall': 0.652, 'f1-score': 0.6093457943925233, 'support': 250}, 'Entailment': {'precision': 0.5953488372093023, 'recall': 0.512, 'f1-score': 0.5505376344086022, 'support': 250}, 'accuracy': 0.582, 'macro avg': {'precision': 0.5836393308853529, 'recall': 0.5820000000000001, 'f1-score': 0.5799417144005627, 'support': 500}, 'weighted avg': {'precision': 0.5836393308853529, 'recall': 0.582, 'f1-score': 0.5799417144005626, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5927272727272728, 'recall': 0.652, 'f1-score': 0.6209523809523809, 'support': 250}, 'Entailment': {'precision': 0.6133333333333333, 'recall': 0.552, 'f1-score': 0.5810526315789474, 'support': 250}, 'accuracy': 0.602, 'macro avg': {'precision': 0.603030303030303, 'recall': 0.6020000000000001, 'f1-score': 0.6010025062656641, 'support': 500}, 'weighted avg': {'precision': 0.603030303030303, 'recall': 0.602, 'f1-score': 0.6010025062656641, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5825825825825826, 'recall': 0.776, 'f1-score': 0.6655231560891938, 'support': 250}, 'Entailment': {'precision': 0.6646706586826348, 'recall': 0.444, 'f1-score': 0.5323741007194245, 'support': 250}, 'accuracy': 0.61, 'macro avg': {'precision': 0.6236266206326087, 'recall': 0.61, 'f1-score': 0.5989486284043091, 'support': 500}, 'weighted avg': {'precision': 0.6236266206326085, 'recall': 0.61, 'f1-score': 0.5989486284043092, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6059479553903345, 'recall': 0.652, 'f1-score': 0.6281310211946051, 'support': 250}, 'Entailment': {'precision': 0.6233766233766234, 'recall': 0.576, 'f1-score': 0.5987525987525988, 'support': 250}, 'accuracy': 0.614, 'macro avg': {'precision': 0.614662289383479, 'recall': 0.614, 'f1-score': 0.6134418099736019, 'support': 500}, 'weighted avg': {'precision': 0.614662289383479, 'recall': 0.614, 'f1-score': 0.613441809973602, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5909090909090909, 'recall': 0.676, 'f1-score': 0.6305970149253732, 'support': 250}, 'Entailment': {'precision': 0.6214953271028038, 'recall': 0.532, 'f1-score': 0.5732758620689655, 'support': 250}, 'accuracy': 0.604, 'macro avg': {'precision': 0.6062022090059473, 'recall': 0.6040000000000001, 'f1-score': 0.6019364384971694, 'support': 500}, 'weighted avg': {'precision': 0.6062022090059475, 'recall': 0.604, 'f1-score': 0.6019364384971694, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5923076923076923, 'recall': 0.616, 'f1-score': 0.6039215686274509, 'support': 250}, 'Entailment': {'precision': 0.6, 'recall': 0.576, 'f1-score': 0.5877551020408163, 'support': 250}, 'accuracy': 0.596, 'macro avg': {'precision': 0.5961538461538461, 'recall': 0.596, 'f1-score': 0.5958383353341337, 'support': 500}, 'weighted avg': {'precision': 0.5961538461538461, 'recall': 0.596, 'f1-score': 0.5958383353341336, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.589041095890411, 'recall': 0.688, 'f1-score': 0.6346863468634687, 'support': 250}, 'Entailment': {'precision': 0.625, 'recall': 0.52, 'f1-score': 0.5676855895196506, 'support': 250}, 'accuracy': 0.604, 'macro avg': {'precision': 0.6070205479452055, 'recall': 0.604, 'f1-score': 0.6011859681915597, 'support': 500}, 'weighted avg': {'precision': 0.6070205479452054, 'recall': 0.604, 'f1-score': 0.6011859681915596, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6081081081081081, 'recall': 0.72, 'f1-score': 0.6593406593406593, 'support': 250}, 'Entailment': {'precision': 0.6568627450980392, 'recall': 0.536, 'f1-score': 0.5903083700440529, 'support': 250}, 'accuracy': 0.628, 'macro avg': {'precision': 0.6324854266030737, 'recall': 0.628, 'f1-score': 0.6248245146923561, 'support': 500}, 'weighted avg': {'precision': 0.6324854266030736, 'recall': 0.628, 'f1-score': 0.6248245146923561, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6133828996282528, 'recall': 0.66, 'f1-score': 0.6358381502890174, 'support': 250}, 'Entailment': {'precision': 0.6320346320346321, 'recall': 0.584, 'f1-score': 0.6070686070686071, 'support': 250}, 'accuracy': 0.622, 'macro avg': {'precision': 0.6227087658314424, 'recall': 0.622, 'f1-score': 0.6214533786788122, 'support': 500}, 'weighted avg': {'precision': 0.6227087658314424, 'recall': 0.622, 'f1-score': 0.6214533786788122, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6120996441281139, 'recall': 0.688, 'f1-score': 0.647834274952919, 'support': 250}, 'Entailment': {'precision': 0.6438356164383562, 'recall': 0.564, 'f1-score': 0.6012793176972281, 'support': 250}, 'accuracy': 0.626, 'macro avg': {'precision': 0.6279676302832351, 'recall': 0.6259999999999999, 'f1-score': 0.6245567963250735, 'support': 500}, 'weighted avg': {'precision': 0.6279676302832351, 'recall': 0.626, 'f1-score': 0.6245567963250734, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6068965517241379, 'recall': 0.704, 'f1-score': 0.6518518518518518, 'support': 250}, 'Entailment': {'precision': 0.6476190476190476, 'recall': 0.544, 'f1-score': 0.5913043478260871, 'support': 250}, 'accuracy': 0.624, 'macro avg': {'precision': 0.6272577996715928, 'recall': 0.624, 'f1-score': 0.6215780998389695, 'support': 500}, 'weighted avg': {'precision': 0.6272577996715928, 'recall': 0.624, 'f1-score': 0.6215780998389695, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6036363636363636, 'recall': 0.664, 'f1-score': 0.6323809523809524, 'support': 250}, 'Entailment': {'precision': 0.6266666666666667, 'recall': 0.564, 'f1-score': 0.5936842105263158, 'support': 250}, 'accuracy': 0.614, 'macro avg': {'precision': 0.6151515151515152, 'recall': 0.614, 'f1-score': 0.6130325814536342, 'support': 500}, 'weighted avg': {'precision': 0.6151515151515152, 'recall': 0.614, 'f1-score': 0.613032581453634, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.61003861003861, 'recall': 0.632, 'f1-score': 0.6208251473477406, 'support': 250}, 'Entailment': {'precision': 0.6182572614107884, 'recall': 0.596, 'f1-score': 0.6069246435845214, 'support': 250}, 'accuracy': 0.614, 'macro avg': {'precision': 0.6141479357246993, 'recall': 0.614, 'f1-score': 0.613874895466131, 'support': 500}, 'weighted avg': {'precision': 0.6141479357246992, 'recall': 0.614, 'f1-score': 0.6138748954661309, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.5859872611464968, 'recall': 0.736, 'f1-score': 0.652482269503546, 'support': 250}, 'Entailment': {'precision': 0.6451612903225806, 'recall': 0.48, 'f1-score': 0.5504587155963303, 'support': 250}, 'accuracy': 0.608, 'macro avg': {'precision': 0.6155742757345387, 'recall': 0.608, 'f1-score': 0.6014704925499381, 'support': 500}, 'weighted avg': {'precision': 0.6155742757345388, 'recall': 0.608, 'f1-score': 0.6014704925499381, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6048951048951049, 'recall': 0.692, 'f1-score': 0.6455223880597015, 'support': 250}, 'Entailment': {'precision': 0.6401869158878505, 'recall': 0.548, 'f1-score': 0.5905172413793105, 'support': 250}, 'accuracy': 0.62, 'macro avg': {'precision': 0.6225410103914777, 'recall': 0.62, 'f1-score': 0.618019814719506, 'support': 500}, 'weighted avg': {'precision': 0.6225410103914776, 'recall': 0.62, 'f1-score': 0.6180198147195061, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6021505376344086, 'recall': 0.672, 'f1-score': 0.6351606805293006, 'support': 250}, 'Entailment': {'precision': 0.6289592760180995, 'recall': 0.556, 'f1-score': 0.5902335456475584, 'support': 250}, 'accuracy': 0.614, 'macro avg': {'precision': 0.615554906826254, 'recall': 0.6140000000000001, 'f1-score': 0.6126971130884296, 'support': 500}, 'weighted avg': {'precision': 0.615554906826254, 'recall': 0.614, 'f1-score': 0.6126971130884294, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6035714285714285, 'recall': 0.676, 'f1-score': 0.6377358490566037, 'support': 250}, 'Entailment': {'precision': 0.6318181818181818, 'recall': 0.556, 'f1-score': 0.5914893617021277, 'support': 250}, 'accuracy': 0.616, 'macro avg': {'precision': 0.6176948051948052, 'recall': 0.6160000000000001, 'f1-score': 0.6146126053793657, 'support': 500}, 'weighted avg': {'precision': 0.6176948051948052, 'recall': 0.616, 'f1-score': 0.6146126053793657, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6088560885608856, 'recall': 0.66, 'f1-score': 0.6333973128598848, 'support': 250}, 'Entailment': {'precision': 0.62882096069869, 'recall': 0.576, 'f1-score': 0.6012526096033403, 'support': 250}, 'accuracy': 0.618, 'macro avg': {'precision': 0.6188385246297878, 'recall': 0.618, 'f1-score': 0.6173249612316125, 'support': 500}, 'weighted avg': {'precision': 0.6188385246297878, 'recall': 0.618, 'f1-score': 0.6173249612316125, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6079136690647482, 'recall': 0.676, 'f1-score': 0.6401515151515152, 'support': 250}, 'Entailment': {'precision': 0.6351351351351351, 'recall': 0.564, 'f1-score': 0.5974576271186439, 'support': 250}, 'accuracy': 0.62, 'macro avg': {'precision': 0.6215244020999416, 'recall': 0.62, 'f1-score': 0.6188045711350796, 'support': 500}, 'weighted avg': {'precision': 0.6215244020999416, 'recall': 0.62, 'f1-score': 0.6188045711350797, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6028368794326241, 'recall': 0.68, 'f1-score': 0.6390977443609023, 'support': 250}, 'Entailment': {'precision': 0.6330275229357798, 'recall': 0.552, 'f1-score': 0.5897435897435898, 'support': 250}, 'accuracy': 0.616, 'macro avg': {'precision': 0.6179322011842019, 'recall': 0.6160000000000001, 'f1-score': 0.6144206670522461, 'support': 500}, 'weighted avg': {'precision': 0.6179322011842019, 'recall': 0.616, 'f1-score': 0.6144206670522461, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6096654275092936, 'recall': 0.656, 'f1-score': 0.6319845857418112, 'support': 250}, 'Entailment': {'precision': 0.6277056277056277, 'recall': 0.58, 'f1-score': 0.6029106029106028, 'support': 250}, 'accuracy': 0.618, 'macro avg': {'precision': 0.6186855276074607, 'recall': 0.618, 'f1-score': 0.617447594326207, 'support': 500}, 'weighted avg': {'precision': 0.6186855276074607, 'recall': 0.618, 'f1-score': 0.6174475943262071, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6066176470588235, 'recall': 0.66, 'f1-score': 0.632183908045977, 'support': 250}, 'Entailment': {'precision': 0.6271929824561403, 'recall': 0.572, 'f1-score': 0.5983263598326359, 'support': 250}, 'accuracy': 0.616, 'macro avg': {'precision': 0.6169053147574819, 'recall': 0.616, 'f1-score': 0.6152551339393064, 'support': 500}, 'weighted avg': {'precision': 0.6169053147574819, 'recall': 0.616, 'f1-score': 0.6152551339393064, 'support': 500}}\n",
      "\n",
      "{'Contradiction': {'precision': 0.6088560885608856, 'recall': 0.66, 'f1-score': 0.6333973128598848, 'support': 250}, 'Entailment': {'precision': 0.62882096069869, 'recall': 0.576, 'f1-score': 0.6012526096033403, 'support': 250}, 'accuracy': 0.618, 'macro avg': {'precision': 0.6188385246297878, 'recall': 0.618, 'f1-score': 0.6173249612316125, 'support': 500}, 'weighted avg': {'precision': 0.6188385246297878, 'recall': 0.618, 'f1-score': 0.6173249612316125, 'support': 500}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Reports for Test Set\")\n",
    "for  epoch in clasification_reports_test.keys():\n",
    "    print(clasification_reports_test[epoch])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-prompt-turbo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
